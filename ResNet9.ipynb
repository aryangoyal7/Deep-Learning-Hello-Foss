{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4tyH9qt7BtmR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JxaW7cAjB1AL"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets --upgrade --quiet\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IKrcmU1GB30o"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'https://www.kaggle.com/datasets/aneesh10/cricket-shot-dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Av9jhhB4t6"
      },
      "outputs": [],
      "source": [
        "od.download(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-PStJ-ZdB4yI"
      },
      "outputs": [],
      "source": [
        "data_dir = './cricket-shot-dataset/data'\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXq8VFtgB41L",
        "outputId": "814deb6f-503b-42f1-8208-dce7549b495a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['legglance-flick', 'drive', 'pullshot', 'sweep']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zb5W-t8xCX9t"
      },
      "outputs": [],
      "source": [
        "dataset = ImageFolder(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bFZDlliwCyiQ"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as tt\n",
        "normalize = tt.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        ")\n",
        "\n",
        "dataset = ImageFolder(data_dir, tt.Compose([tt.Resize(64), \n",
        "                                            tt.RandomCrop(64), \n",
        "                                            tt.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73prC60eCyta",
        "outputId": "8b63fddd-d6ff-4fe3-80f8-cdeb60ceab7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4252, 472)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_pct = 0.1\n",
        "val_size = int(val_pct * len(dataset))\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_size, val_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCt4e48LDARQ",
        "outputId": "74ea99c1-74a2-4904-d144-2ad8ac8f026f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4252, 472)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, valid_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(valid_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-7J1v44DFup",
        "outputId": "94ee6bd4-5c8d-495a-bead-b95f2a06c885"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dl = DataLoader(train_ds, \n",
        "                      batch_size, \n",
        "                      shuffle=True, \n",
        "                      num_workers=4, \n",
        "                      pin_memory=True)\n",
        "\n",
        "valid_dl = DataLoader(valid_ds, \n",
        "                    batch_size, \n",
        "                    num_workers=4, \n",
        "                    pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TvEI5m6UDIfu"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        # Input: 128 x 3 x 64 x 64\n",
        "        self.conv1 = conv_block(in_channels, 64) # 128 x 64 x 64 x 64\n",
        "        self.conv2 = conv_block(64, 128, pool=True) # 128 x 128 x 32 x 32\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), # 128 x 128 x 32 x 32\n",
        "                                  conv_block(128, 128)) # 128 x 128 x 32 x 32\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True) # 128 x 256 x 16 x 16\n",
        "        self.conv4 = conv_block(256, 512, pool=True) # 128 x 512 x 8 x 8 \n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), # 128 x 512 x 8 x 8 \n",
        "                                  conv_block(512, 512)) # 128 x 512 x 8 x 8 \n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1), # 128 x 512 x 1 x 1 \n",
        "                                        nn.Flatten(), # 128 x 512\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9F7xKuj_DRBV"
      },
      "outputs": [],
      "source": [
        "num_classes = 4\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 0.005\n",
        "in_channels = 3\n",
        "model = ResNet9(3,4).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1TOrhUqDVYx",
        "outputId": "3da0fcee-20b0-49ed-fdfa-c98c4f2d4489"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [34/34], Loss: 3.0790\n",
            "Accuracy of the network on the 5000 validation images: 27.54237288135593 %\n",
            "Epoch [2/20], Step [34/34], Loss: 1.3549\n",
            "Accuracy of the network on the 5000 validation images: 28.8135593220339 %\n",
            "Epoch [3/20], Step [34/34], Loss: 1.5040\n",
            "Accuracy of the network on the 5000 validation images: 34.74576271186441 %\n",
            "Epoch [4/20], Step [34/34], Loss: 1.4431\n",
            "Accuracy of the network on the 5000 validation images: 38.983050847457626 %\n",
            "Epoch [5/20], Step [34/34], Loss: 1.4845\n",
            "Accuracy of the network on the 5000 validation images: 42.16101694915254 %\n",
            "Epoch [6/20], Step [34/34], Loss: 1.2198\n",
            "Accuracy of the network on the 5000 validation images: 39.61864406779661 %\n",
            "Epoch [7/20], Step [34/34], Loss: 1.4960\n",
            "Accuracy of the network on the 5000 validation images: 48.30508474576271 %\n",
            "Epoch [8/20], Step [34/34], Loss: 1.1226\n",
            "Accuracy of the network on the 5000 validation images: 43.644067796610166 %\n",
            "Epoch [9/20], Step [34/34], Loss: 1.0936\n",
            "Accuracy of the network on the 5000 validation images: 45.1271186440678 %\n",
            "Epoch [10/20], Step [34/34], Loss: 1.4316\n",
            "Accuracy of the network on the 5000 validation images: 51.90677966101695 %\n",
            "Epoch [11/20], Step [34/34], Loss: 1.0024\n",
            "Accuracy of the network on the 5000 validation images: 53.601694915254235 %\n",
            "Epoch [12/20], Step [34/34], Loss: 0.6813\n",
            "Accuracy of the network on the 5000 validation images: 59.110169491525426 %\n",
            "Epoch [13/20], Step [34/34], Loss: 0.7405\n",
            "Accuracy of the network on the 5000 validation images: 52.54237288135593 %\n",
            "Epoch [14/20], Step [34/34], Loss: 0.9952\n",
            "Accuracy of the network on the 5000 validation images: 64.40677966101696 %\n",
            "Epoch [15/20], Step [34/34], Loss: 0.7834\n",
            "Accuracy of the network on the 5000 validation images: 66.52542372881356 %\n",
            "Epoch [16/20], Step [34/34], Loss: 0.5098\n",
            "Accuracy of the network on the 5000 validation images: 69.91525423728814 %\n",
            "Epoch [17/20], Step [34/34], Loss: 0.6582\n",
            "Accuracy of the network on the 5000 validation images: 69.27966101694915 %\n",
            "Epoch [18/20], Step [34/34], Loss: 0.5247\n",
            "Accuracy of the network on the 5000 validation images: 68.22033898305085 %\n",
            "Epoch [19/20], Step [34/34], Loss: 0.4296\n",
            "Accuracy of the network on the 5000 validation images: 74.78813559322033 %\n",
            "Epoch [20/20], Step [34/34], Loss: 1.0573\n",
            "Accuracy of the network on the 5000 validation images: 75.63559322033899 %\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_dl)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dl):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_dl:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
